import json

notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ol√°, Dev! Bem-vindo(a) ao seu Workshop de RAG!\\n",
                "\\n",
                "**Seu papel:** Engenheiro(a) de IA J√∫nior\\n",
                "**Sua miss√£o:** Construir e entender um chatbot para a **Lanchonete Candango Lanches** usando o poder de LLMs com uma base de conhecimento pr√≥pria.\\n",
                "\\n",
                "Neste notebook, vamos explorar passo a passo como construir um chatbot que n√£o apenas conversa, mas que tamb√©m consulta documentos para dar respostas precisas. Este processo √© chamado de **RAG (Retrieval-Augmented Generation)**, ou Gera√ß√£o Aumentada por Recupera√ß√£o.\\n",
                "\\n",
                "**O que vamos aprender:**\\n",
                "1.  **Configurar o ambiente** com as bibliotecas essenciais como LangChain e Google Generative AI.\\n",
                "2.  **Construir um pipeline de RAG completo:** Desde carregar documentos at√© a gera√ß√£o de respostas baseadas neles.\\n",
                "3.  **Interagir com o chatbot** de forma pr√°tica.\\n",
                "4.  **Explorar t√≥picos avan√ßados (B√¥nus):** Veremos como usar LLMs para tarefas de classifica√ß√£o (triagem) e daremos dicas valiosas para levar seu projeto para o pr√≥ximo n√≠vel (produ√ß√£o).\\n",
                "\\n",
                "Vamos come√ßar essa jornada!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parte 1: Configura√ß√£o do Ambiente\\n",
                "\\n",
                "Nesta primeira parte, vamos preparar nosso ambiente de desenvolvimento, instalando as bibliotecas necess√°rias e configurando o acesso ao modelo de linguagem do Google, o Gemini."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instalando todas as depend√™ncias necess√°rias de uma s√≥ vez.\\n",
                "# Usamos o '-q' para uma instala√ß√£o mais limpa (quiet).\\n",
                "!pip install -q langchain langchain-google-genai google-generativeai langchain_community faiss-cpu langchain-text-splitters pymupdf reportlab"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Organizando todas as importa√ß√µes em um √∫nico lugar para maior clareza.\\n",
                "import os\\n",
                "from pathlib import Path\\n",
                "from typing import Literal, List, Dict, Any\\n",
                "\\n",
                "# LangChain e Google Generative AI\\n",
                "from google.colab import userdata\\n",
                "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\\n",
                "from langchain.chains.combine_documents import create_stuff_documents_chain\\n",
                "from langchain_core.prompts import ChatPromptTemplate\\n",
                "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\\n",
                "from langchain_core.output_parsers import StrOutputParser\\n",
                "from langchain_core.messages import SystemMessage, HumanMessage\\n",
                "\\n",
                "# Carregamento e Divis√£o de Documentos\\n",
                "from langchain_community.document_loaders import PyMuPDFLoader\\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\\n",
                "\\n",
                "# Vector Store e Retriever\\n",
                "from langchain_community.vectorstores import FAISS\\n",
                "\\n",
                "# Pydantic para Modelagem de Dados (usado na se√ß√£o b√¥nus)\\n",
                "from pydantic import BaseModel, Field\\n",
                "\\n",
                "# ReportLab para criar PDFs (usado para gerar arquivos de exemplo)\\n",
                "from reportlab.pdfgen import canvas\\n",
                "from reportlab.lib.pagesizes import letter"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.1 - Configurando a Chave de API do Gemini\\n",
                "\\n",
                "Para usar os modelos do Google, voc√™ precisa de uma chave de API. A forma mais segura de fazer isso no Google Colab √© usando o Gerenciador de Segredos.\\n",
                "\\n",
                "1.  Clique no √≠cone de chave üîë no painel esquerdo.\\n",
                "2.  Adicione um novo segredo com o nome `GEMINI_API_KEY`.\\n",
                "3.  Cole a sua chave de API do Google AI Studio no campo de valor.\\n",
                "4.  Certifique-se de que o acesso ao notebook est√° habilitado.\\n",
                "\\n",
                "O c√≥digo abaixo acessar√° essa chave de forma segura."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\\n",
                "\\n",
                "# Inicializando o modelo de linguagem (LLM) que usaremos em todo o notebook.\\n",
                "# O 'temperature' controla a criatividade do modelo (0 = mais determin√≠stico, 1 = mais criativo).\\n",
                "llm = ChatGoogleGenerativeAI(\\n",
                "    model=\\\"gemini-1.5-flash\\\",\\n",
                "    temperature=0.2,\\n",
                "    google_api_key=GOOGLE_API_KEY\\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parte 2: Construindo o Pipeline de RAG\\n",
                "\\n",
                "Agora vamos ao cora√ß√£o do nosso projeto: o pipeline de RAG. Ele permitir√° que nosso chatbot consulte documentos para formular suas respostas."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 - Preparando a Base de Conhecimento (Nossos Documentos)\\n",
                "\\n",
                "Um sistema RAG precisa de uma fonte de conhecimento. No nosso caso, ser√£o as pol√≠ticas da lanchonete em formato PDF.\\n",
                "\\n",
                "Para garantir que este notebook funcione para qualquer pessoa, vamos criar programaticamente alguns arquivos PDF de exemplo. Em um projeto real, voc√™ usaria seus pr√≥prios arquivos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\\\"Criando arquivos PDF de exemplo para a base de conhecimento...\\\")\\n",
                "\\n",
                "conteudo_atendimento = \\\"\\\"\\\"Pol√≠tica de Atendimento ao Cliente da Candango Lanches\\n",
                "- Boas-Vindas: A equipe deve sempre cumprimentar o cliente de forma amig√°vel.\\n",
                "- Card√°pio: O colaborador deve conhecer o card√°pio para descrever os itens.\\n",
                "- Pedidos: Todos os pedidos devem ser conferidos com o cliente.\\n",
                "- Feedback: Reclama√ß√µes e sugest√µes devem ser ouvidas com aten√ß√£o e encaminhadas ao gerente.\\n",
                "\\\"\\\"\\\"\\n",
                "\\n",
                "conteudo_seguranca = \\\"\\\"\\\"Pol√≠tica de Seguran√ßa e Uso de Equipamentos da Candango Lanches\\n",
                "- Equipamentos de Prote√ß√£o: O uso de cal√ßados antiderrapantes √© obrigat√≥rio.\\n",
                "- Manuseio de Facas: As facas devem ser guardadas em suportes espec√≠ficos.\\n",
                "- M√°quinas: O uso de cortadores e moedores s√≥ √© permitido ap√≥s treinamento.\\n",
                "- Limpeza de Fritadeiras: √â proibido limpar fritadeiras com √≥leo quente.\\n",
                "\\\"\\\"\\\"\\n",
                "\\n",
                "conteudo_higiene = \\\"\\\"\\\"Pol√≠tica de Higiene e Seguran√ßa Alimentar da Candango Lanches\\n",
                "- Limpeza: Equipamentos e bancadas devem ser higienizados diariamente.\\n",
                "- Armazenamento: Alimentos perec√≠veis devem ser armazenados em temperaturas adequadas.\\n",
                "- Uniforme: √â obrigat√≥rio o uso de uniforme limpo, touca e luvas.\\n",
                "- Contamina√ß√£o Cruzada: Evitar o contato entre alimentos crus e cozidos.\\n",
                "\\\"\\\"\\\"\\n",
                "\\n",
                "arquivos_pdf = {\\n",
                "    \\\"Politica_Atendimento.pdf\\\": conteudo_atendimento,\\n",
                "    \\\"Politica_Seguranca.pdf\\\": conteudo_seguranca,\\n",
                "    \\\"Politica_Higiene.pdf\\\": conteudo_higiene\\n",
                "}\\n",
                "\\n",
                "for nome_arquivo, conteudo in arquivos_pdf.items():\\n",
                "    c = canvas.Canvas(nome_arquivo, pagesize=letter)\\n",
                "    textobject = c.beginText(50, 750)\\n",
                "    textobject.setFont(\\\"Times-Roman\\\", 12)\\n",
                "    for line in conteudo.splitlines():\\n",
                "        textobject.textLine(line)\\n",
                "    c.drawText(textobject)\\n",
                "    c.save()\\n",
                "\\n",
                "print(f\\\"{len(arquivos_pdf)} arquivos PDF de exemplo criados com sucesso!\\\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 - Carregando e Dividindo os Documentos\\n",
                "\\n",
                "Com os arquivos prontos, o primeiro passo do pipeline √© carreg√°-los e dividi-los em peda√ßos menores (chunks). Por que dividir?\\n",
                "- **Contexto Limitado:** Modelos de linguagem t√™m um limite de quantos tokens conseguem processar de uma vez.\\n",
                "- **Busca Eficiente:** √â mais f√°cil e r√°pido encontrar um trecho espec√≠fico de informa√ß√£o em peda√ßos menores do que em um documento inteiro."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "docs = []\\n",
                "for pdf_path in Path(\\\"./\\\").glob(\\\"*.pdf\\\"):\\n",
                "    try:\\n",
                "        loader = PyMuPDFLoader(str(pdf_path))\\n",
                "        docs.extend(loader.load())\\n",
                "        print(f\\\"Documento carregado: {pdf_path}\\\")\\n",
                "    except Exception as e:\\n",
                "        print(f\\\"Erro ao carregar {pdf_path}: {e}\\\")\\n",
                "\\n",
                "# Dividindo os documentos em chunks.\\n",
                "# chunk_size: o tamanho m√°ximo de cada peda√ßo.\\n",
                "# chunk_overlap: uma pequena sobreposi√ß√£o de texto entre os chunks para n√£o perder o contexto.\\n",
                "splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)\\n",
                "chunks = splitter.split_documents(docs)\\n",
                "\\n",
                "print(f\\\"\\nTotal de documentos: {len(docs)}\\\")\\n",
                "print(f\\\"Total de chunks gerados: {len(chunks)}\\\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 - Criando a Mem√≥ria Vetorial (Embeddings & Vector Store)\\n",
                "\\n",
                "Agora, precisamos converter nossos `chunks` de texto em vetores num√©ricos, ou **embeddings**. Embeddings capturam o significado sem√¢ntico do texto, permitindo que comparemos o qu√£o \\\"pr√≥ximas\\\" s√£o duas frases em termos de significado.\\n",
                "\\n",
                "Esses vetores ser√£o armazenados em um **Vector Store**, um tipo de banco de dados otimizado para buscas de similaridade. Usaremos o **FAISS**, uma biblioteca do Facebook AI para buscas eficientes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inicializa o modelo de embeddings do Google.\\n",
                "embeddings = GoogleGenerativeAIEmbeddings(\\n",
                "    model=\\\"models/embedding-001\\\",\\n",
                "    google_api_key=GOOGLE_API_KEY\\n",
                ")\\n",
                "\\n",
                "# Cria o banco de dados vetorial FAISS a partir dos chunks e embeddings.\\n",
                "# Adicionamos uma verifica√ß√£o para garantir que temos chunks antes de criar o √≠ndice.\\n",
                "if chunks:\\n",
                "    vectorstore = FAISS.from_documents(chunks, embeddings)\\n",
                "    print(\\\"Banco de dados vetorial FAISS criado com sucesso!\\\")\\n",
                "else:\\n",
                "    print(\\\"Nenhum chunk foi gerado. O banco de dados vetorial n√£o pode ser criado.\\\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.4 - O Recuperador (The Retriever)\\n",
                "\\n",
                "O Retriever √© o componente que, dada uma pergunta do usu√°rio, faz a busca no Vector Store e \\\"recupera\\\" os chunks mais relevantes para responder √†quela pergunta."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Converte o vectorstore em um retriever.\\n",
                "# search_type=\\\"similarity_score_threshold\\\": busca por similaridade e filtra por um score m√≠nimo.\\n",
                "# search_kwargs: define o score m√≠nimo (0.3) e o n√∫mero m√°ximo de documentos a retornar (k=4).\\n",
                "if 'vectorstore' in locals() and vectorstore:\\n",
                "  retriever = vectorstore.as_retriever(\\n",
                "      search_type=\\\"similarity_score_threshold\\\",\\n",
                "      search_kwargs={\\\"score_threshold\\\": 0.3, \\\"k\\\": 4}\\n",
                "  )\\n",
                "  print(\\\"Retriever configurado.\\\")\\n",
                "else:\\n",
                "  retriever = None\\n",
                "  print(\\\"Retriever n√£o p√¥de ser configurado pois o vectorstore n√£o foi criado.\\\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.5 - Criando a Cadeia de RAG com LCEL\\n",
                "\\n",
                "Agora, vamos juntar tudo! Usaremos a **LangChain Expression Language (LCEL)** para criar uma \\\"cadeia\\\" de opera√ß√µes:\\n",
                "\\n",
                "1.  A pergunta do usu√°rio √© passada para o `retriever`.\\n",
                "2.  O `retriever` busca os `chunks` relevantes (o **contexto**).\\n",
                "3.  A pergunta original e o contexto recuperado s√£o passados para um **prompt**.\\n",
                "4.  O prompt preenchido √© enviado para o **LLM**.\\n",
                "5.  O LLM gera a resposta final."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Template do prompt que instrui o LLM sobre como agir.\\n",
                "prompt_rag = ChatPromptTemplate.from_messages([\\n",
                "    (\\\"system\\\",\\n",
                "     \\\"Voc√™ √© o assistente virtual da Lanchonete Candango Lanches. \\\"\\n",
                "     \\\"Sua fun√ß√£o √© responder a perguntas sobre as pol√≠ticas da lanchonete com base no contexto fornecido. \\\"\\n",
                "     \\\"Seja amig√°vel, prestativo e use uma linguagem casual e acolhedora. \\\"\\n",
                "     \\\"Responda APENAS com base nas informa√ß√µes do contexto abaixo.\\\\n\\\\n\\\"\\n",
                "     \\\"Contexto:\\\\n{context}\\\"\\n",
                "     ),\\n",
                "    (\\\"human\\\", \\\"Pergunta: {question}\\\"),\\n",
                "])\\n",
                "\\n",
                "# Cria a cadeia que combina o LLM e o prompt.\\n",
                "chain_de_geracao = create_stuff_documents_chain(llm, prompt_rag)\\n",
                "\\n",
                "# Cria a cadeia de RAG completa usando LCEL.\\n",
                "# RunnableParallel permite que o retriever e a pergunta sejam processados em paralelo.\\n",
                "# .assign(answer=...) adiciona a resposta gerada ao resultado final.\\n",
                "if retriever:\\n",
                "  rag_chain = RunnableParallel(\\n",
                "      {\\\"context\\\": retriever, \\\"question\\\": RunnablePassthrough()}\\n",
                "  ).assign(answer = chain_de_geracao | StrOutputParser())\\n",
                "  print(\\\"Cadeia de RAG criada com sucesso!\\\")\\n",
                "else:\\n",
                "  rag_chain = None\\n",
                "  print(\\\"Cadeia de RAG n√£o foi criada devido a erro no retriever.\\\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.6 - A Fun√ß√£o de Consulta Final\\n",
                "\\n",
                "Para facilitar a intera√ß√£o, vamos encapsular a chamada da `rag_chain` em uma fun√ß√£o que tamb√©m formata a sa√≠da, incluindo as fontes consultadas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def perguntar_ao_rag(pergunta: str) -> Dict[str, Any]:\\n",
                "    \\\"\\\"\\\"\\n",
                "    Recebe uma pergunta, executa o pipeline de RAG e retorna a resposta com cita√ß√µes.\\n",
                "    \\\"\\\"\\\"\\n",
                "    if not rag_chain:\\n",
                "        return {\\n",
                "            \\\"answer\\\": \\\"Desculpe, o sistema de busca n√£o est√° operacional.\\\",\\n",
                "            \\\"citacoes\\\": [],\\n",
                "            \\\"contexto_encontrado\\\": False\\n",
                "        }\\n",
                "        \\n",
                "    resultado = rag_chain.invoke(pergunta)\\n",
                "    \\n",
                "    # Usamos .get() para acessar as chaves do dicion√°rio de forma segura.\\n",
                "    answer = resultado.get('answer', 'N√£o foi poss√≠vel obter uma resposta.')\\n",
                "    docs_relacionados = resultado.get('context', [])\\n",
                "\\n",
                "    # Formata as cita√ß√µes dos documentos recuperados.\\n",
                "    citacoes = [Path(doc.metadata.get('source', '')).name for doc in docs_relacionados]\\n",
                "    \\n",
                "    # L√≥gica de controle: se nenhum documento foi encontrado, a resposta √© gen√©rica.\\n",
                "    if not docs_relacionados:\\n",
                "        return {\\n",
                "            \\\"answer\\\": \\\"Desculpe, n√£o encontrei informa√ß√µes sobre isso em nossas pol√≠ticas.\\\",\\n",
                "            \\\"citacoes\\\": [],\\n",
                "            \\\"contexto_encontrado\\\": False\\n",
                "        }\\n",
                "    \\n",
                "    return {\\n",
                "        \\\"answer\\\": answer,\\n",
                "        \\\"citacoes\\\": list(set(citacoes)), # Remove duplicatas\\n",
                "        \\\"contexto_encontrado\\\": True\\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parte 3: Interagindo com o Chatbot\\n",
                "\\n",
                "Tudo pronto! Agora vamos criar um loop interativo para conversar com nosso chatbot."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chatbot_lanchonete():\\n",
                "    print(\\\"\\\\n--- Bem-vindo ao Chatbot da Candango Lanches! ---\\\")\\n",
                "    print(\\\"Posso responder perguntas sobre nossas pol√≠ticas de atendimento, seguran√ßa e higiene.\\\")\\n",
                "    print(\\\"Digite 'sair' para encerrar.\\\")\\n",
                "    print(\\\"-\\\" * 50)\\n",
                "\\n",
                "    while True:\\n",
                "        pergunta_cliente = input(\\\"Sua pergunta: \\\")\\n",
                "        if pergunta_cliente.lower() == 'sair':\\n",
                "            print(\\\"\\\\nObrigado por usar o Chatbot da Candango Lanches!\\\")\\n",
                "            break\\n",
                "\\n",
                "        resultado = perguntar_ao_rag(pergunta_cliente)\\n",
                "\\n",
                "        print(\\\"\\\\nResposta do Chatbot:\\\")\\n",
                "        print(resultado['answer'])\\n",
                "\\n",
                "        if resultado['contexto_encontrado'] and resultado['citacoes']:\\n",
                "            print(\\\"\\\\nFontes consultadas:\\\")\\n",
                "            for citacao in resultado['citacoes']:\\n",
                "                print(f\\\"- {citacao}\\\")\\n",
                "        \\n",
                "        print(\\\"-\\\" * 50)\\n",
                "\\n",
                "# Para iniciar o chatbot, remova o coment√°rio da linha abaixo e execute a c√©lula.\\n",
                "# chatbot_lanchonete()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parte 4: T√≥picos Adicionais & Boas Pr√°ticas (B√¥nus)\\n",
                "\\n",
                "Parab√©ns por chegar at√© aqui! Agora que o pipeline de RAG est√° funcional, vamos explorar algumas ideias que estavam no seu notebook original e como podemos lev√°-las adiante."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 - O Experimento de Triagem (Structured Output)\\n",
                "\\n",
                "Seu notebook original continha um experimento interessante para **triagem** de mensagens, usando a biblioteca Pydantic para for√ßar o LLM a retornar uma sa√≠da em um formato JSON espec√≠fico. Isso √© chamado de **Structured Output** e √© extremamente √∫til!\\n",
                "\\n",
                "Vamos manter seu c√≥digo original aqui como um exemplo de caso de uso. Ele poderia ser o primeiro passo em um fluxo mais complexo: primeiro, um LLM classifica a mensagem do usu√°rio (triagem) e, depois, decide se deve acionar o RAG, encaminhar para um humano ou outra a√ß√£o.\\n",
                "\\n",
                "**Ponto de aprendizado:** No seu c√≥digo original, o LLM era inicializado dentro da fun√ß√£o `triagem`. Isso funciona, mas √© ineficiente, pois cria um novo objeto a cada chamada. Uma boa pr√°tica seria inicializar o LLM uma vez e reutiliz√°-lo, como fizemos com o `llm` principal no in√≠cio deste notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "TRIAGEM_PROMPT = \\\"\\\"\\\"Voc√™ √© o assistente virtual da Lanchonete Candango Lanches.\\n",
                "Sua fun√ß√£o √© classificar as mensagens dos clientes de acordo com a a√ß√£o necess√°ria e retornar SOMENTE um JSON com:\\\\n\\\\n\\n",
                "{\\\\n\\\\n\\n",
                "    \\\"decisao\\\": \\\"RESPONDER_INFO\\\" | \\\"ENCAMINHAR_ATENDENTE\\\" | \\\"PEDIDO_ESPECIAL\\\",\\\\n\\\\n\\n",
                "    \\\"urgencia\\\": \\\"BAIXA\\\" | \\\"MEDIA\\\" | \\\"ALTA\\\"\\\\n\\\\n\\n",
                "}\\\\n\\\\n\\n",
                "Regras:\\\\n\\\\n\\n",
                "- **RESPONDER_INFO**: Perguntas claras sobre o card√°pio, hor√°rios, ingredientes (Ex: \\\"Qual o pre√ßo do X-Bacon?\\\")\\\\n\\\\n\\n",
                "- **ENCAMINHAR_ATENDENTE**: Mensagens complexas, reclama√ß√µes, ou pedidos de informa√ß√£o muito espec√≠ficos que n√£o est√£o nos documentos (Ex: \\\"Tive um problema com meu pedido.\\\")\\\\n\\\\n\\n",
                "- **PEDIDO_ESPECIAL**: Solicita√ß√µes fora do padr√£o do card√°pio ou que exigem confirma√ß√£o (Ex: \\\"Posso pedir um sandu√≠che sem cebola?\\\").\\\\n\\\\n\\n",
                "Analise a mensagem e decida a a√ß√£o mais apropriada.\\\"\\\"\\\"\\n",
                "\\n",
                "class TriagemOut(BaseModel):\\n",
                "    decisao: Literal[\\\"RESPONDER_INFO\\\", \\\"ENCAMINHAR_ATENDENTE\\\", \\\"PEDIDO_ESPECIAL\\\"] = Field(\\n",
                "        description=\\\"Decis√£o da triagem.\\\"\\n",
                "    )\\n",
                "    urgencia: Literal[\\\"BAIXA\\\", \\\"MEDIA\\\", \\\"ALTA\\\"] = Field(\\n",
                "        description=\\\"N√≠vel de urg√™ncia da solicita√ß√£o.\\\"\\n",
                "    )\\n",
                "\\n",
                "# Mantendo a estrutura original do seu c√≥digo para fins did√°ticos.\\n",
                "def triagem(mensagem: str) -> Dict:\\n",
                "  # Ponto de aprendizado: inicializar o LLM aqui √© ineficiente.\\n",
                "  # O ideal seria reutilizar uma inst√¢ncia j√° criada.\\n",
                "  llm_triagem = ChatGoogleGenerativeAI(\\n",
                "      model=\\\"gemini-1.5-flash\\\",\\n",
                "      temperature=0.0, # Temperatura 0 para ser mais determin√≠stico na classifica√ß√£o\\n",
                "      google_api_key=GOOGLE_API_KEY\\n",
                "  ).with_structured_output(TriagemOut)\\n",
                "\\n",
                "  saida: TriagemOut = llm_triagem.invoke([\\n",
                "      SystemMessage(content=TRIAGEM_PROMPT),\\n",
                "      HumanMessage(content=mensagem)\\n",
                "  ])\\n",
                "\\n",
                "  return saida.model_dump()\\n",
                "\\n",
                "# Testando a fun√ß√£o de triagem\\n",
                "print(\\\"--- Testando o Experimento de Triagem ---\\\")\\n",
                "mensagem_teste = \\\"Tive um problema com a entrega, meu sandu√≠che veio frio.\\\"\\n",
                "resultado_triagem = triagem(mensagem_teste)\\n",
                "print(f\\\"Mensagem: '{mensagem_teste}'\\\")\\n",
                "print(f\\\"Resultado da triagem: {resultado_triagem}\\\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 - Sugest√µes de Inova√ß√£o e Pr√≥ximos Passos\\n",
                "\\n",
                "Excelente trabalho! Agora que voc√™ tem um pipeline de RAG funcional, o c√©u √© o limite. Aqui est√£o algumas ideias para evoluir seu projeto:\\n",
                "\\n",
                "1.  **Usar LangGraph para Fluxos Complexos:** O experimento de triagem √© um exemplo perfeito. E se, ap√≥s a triagem, o sistema decidisse autonomamente se deve chamar o RAG ou fazer outra coisa? **LangGraph** √© a ferramenta ideal para isso. Com ele, voc√™ pode construir fluxos de estado (grafos) onde cada n√≥ √© uma a√ß√£o (triar, buscar no RAG, pedir mais informa√ß√µes ao usu√°rio) e as arestas s√£o as condi√ß√µes para ir de um estado para outro. √â o pr√≥ximo passo para criar agentes mais aut√¥nomos.\\n",
                "\\n",
                "2.  **Adicionar Mem√≥ria:** Para que o chatbot se lembre de intera√ß√µes passadas na mesma conversa, voc√™ pode adicionar um componente de **mem√≥ria** (dispon√≠vel no LangChain). Isso permite que o usu√°rio fa√ßa perguntas de acompanhamento sem precisar repetir o contexto.\\n",
                "\\n",
                "3.  **Avalia√ß√£o Autom√°tica de Respostas:** Como saber se o seu RAG est√° melhorando? Voc√™ pode criar um sistema de avalia√ß√£o (ex: usando o pr√≥prio LLM para comparar a resposta gerada com uma resposta \\\"padr√£o\\\") para medir a qualidade e a relev√¢ncia das respostas. Frameworks como o LangSmith s√£o √≥timos para isso.\\n",
                "\\n",
                "4.  **Feedback Loop com Usu√°rios:** Adicione uma forma simples de o usu√°rio avaliar a resposta (üëç/üëé). Esse feedback √© ouro! Ele pode ser usado para identificar respostas ruins e at√© para re-treinar ou ajustar seu sistema."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 - Dicas para Implementa√ß√£o e Produ√ß√£o\\n",
                "\\n",
                "Levar um projeto de IA do notebook para o mundo real requer algumas pr√°ticas importantes:\\n",
                "\\n",
                "1.  **Exporta√ß√£o do Chatbot:**\\n",
                "    *   **API com FastAPI ou Flask:** A maneira mais comum de colocar seu chatbot em produ√ß√£o √© envolv√™-lo em uma API. FastAPI √© uma excelente escolha por sua performance e facilidade de uso.\\n",
                "    *   **Interface com Streamlit ou Gradio:** Para criar uma interface de demonstra√ß√£o r√°pida e interativa, bibliotecas como Streamlit e Gradio s√£o perfeitas.\\n",
                "    *   **Docker:** Empacote sua aplica√ß√£o e todas as suas depend√™ncias em um cont√™iner Docker. Isso garante que ela funcione de forma consistente em qualquer ambiente.\\n",
                "\\n",
                "2.  **Integra√ß√£o com Fontes Externas:**\\n",
                "    *   Seu Vector Store FAISS atual vive na mem√≥ria. Para produ√ß√£o, voc√™ vai querer um banco de dados vetorial persistente e escal√°vel. Algumas op√ß√µes populares s√£o **Pinecone**, **Weaviate** e **Postgres com a extens√£o `pgvector`**.\\n",
                "\\n",
                "3.  **Boas Pr√°ticas de C√≥digo:**\\n",
                "    *   **Modulariza√ß√£o:** Em vez de um √∫nico notebook, separe seu c√≥digo em m√≥dulos Python (`.py`). Por exemplo, um arquivo para o pipeline de RAG, outro para a configura√ß√£o do LLM, e um `main.py` para a sua API FastAPI.\\n",
                "    *   **Gerenciamento de Segredos:** Nunca deixe chaves de API no c√≥digo. Use vari√°veis de ambiente (com `python-dotenv`) ou um servi√ßo de gerenciamento de segredos do seu provedor de nuvem (AWS Secrets Manager, Google Secret Manager)."
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

with open("notebook_refatorado.ipynb", "w", encoding='utf-8') as f:
    json.dump(notebook, f, indent=2, ensure_ascii=False)

print("Notebook 'notebook_refatorado.ipynb' foi criado com sucesso.")
